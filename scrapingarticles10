import requests
from bs4 import BeautifulSoup
import operator

month = ["January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"]


def usatoday():
	titles = []
	links = []
	dates = []
	authors = []
	contents = []
	

	# page = requests.get('https://www.usatoday.com/search/?q=Korea')
	# soup = BeautifulSoup(page.content, 'html.parser')
	#usatoday = soup.select('div.gnt_se_hl')
	
	for page in range (1,4):
		changepage = "https://www.usatoday.com/search/?q=Korea&page="
		changepage += str(page)
		#print("page = ",changepage)
		page = requests.get(changepage)
		soup = BeautifulSoup(page.content, 'html.parser')
		usatoday = soup.select('div.gnt_se_hl')
		
		for title in usatoday:
			#키워드 필터링
			#if('Korea' in title.text):
			#if (title.parent.parent.get('href').find("/world/")!=-1):
			#print(title.parent.parent.get('href'))
			
			titles.append(title.text)
			links.append(title.parent.parent.get('href'))
        
    		#이 화면에서 저자, 날짜 빼오기
			dtby = title.parent.find(attrs={"class": "gnt_se_dtby"})
			if (dtby!=None):
				#print("******************************",dtby.get('data-c-by'),"******************************")
				#print("******************************",dtby.get('data-c-dt'),"******************************")
				authors.append(dtby.get('data-c-by'))
				dates.append(dtby.get('data-c-dt'))
			elif (dtby==None):
				dtby = title.parent.parent.find(attrs={"class": "gnt_se_dt"})
				dates.append(dtby.get('data-c-dt'))
				# - 저자가 표기되어 있지 않음.
				authors.append("Unknown")
			
			#내용을 긁어오기 위해 기사를 눌렀을 때 나오는 링크로 들어갑니다.
			deeppage = 'https://usatoday.com'
			deeppage += title.parent.parent.get('href')
			page = requests.get(deeppage)
			soup = BeautifulSoup(page.content, 'html.parser')
			num = 1;
			content = ""
			
			attrsforcon = 'speakable-p-1 p-text'
			contentf = soup.find(attrs={"class": attrsforcon})
			
			#첫번째 문장 스크래핑
			while (contentf!=None):
				content+=contentf.text
				#print(contents.text)
				num+=1
				attrsforcon = 'speakable-p-'
				attrsforcon += str(num)
				attrsforcon += ' p-text'
				contentf = soup.find(attrs={"class": attrsforcon})

			#두번쨰 문장 스크래핑
			for contentf in soup.find_all(attrs={"class": "p-text"}):
				content+=contentf.text
				#print(contents.text)
				num+=1

			#word count 시작
			
			wordlist = content.split()
			worddictionary = {}
			
			for word in wordlist:
				if word in worddictionary:
					#Increase
					worddictionary[word] += 1
				else:
					#add to the dictionary
					worddictionary[word] = 1
			
			sortedwords = sorted(worddictionary.items(), key=operator.itemgetter(1), reverse=True)
			
			print(sortedwords)
				
	
	#print(titles)
	#print(links)
	#print(dates)
	#print(authors)
	
	
def nypost():
	titles = []
	links = []
	dates = []
	authors = []
	contents = []

	#page = requests.get('https://nypost.com/search/korea/page/1')
	#soup = BeautifulSoup(page.content, 'html.parser')
	#nypost = soup.select('div.entry-header > h3 > a')
	
	for page in range(1,11):
		#find title & change page
		changepage = "https://nypost.com/search/korea/page/"
		changepage+=str(page)
		page = requests.get(changepage)
		soup = BeautifulSoup(page.content, 'html.parser')
		nypost = soup.select('div.entry-header > h3 > a')
		
		#print("Search ",changepage)
		
		for title in nypost:
			#키워드 필터링
			if('Korea' in title.text):
				#if (title.parent.parent.get('href').find("/world/")!=-1):
				titles.append(title.text)
				links.append(title.get('href'))
			
				#이 화면에서 저자, 날짜 빼오기
				count = -1
				cut = 0;
				author = ""
				date = ""
				dtby = title.parent.parent.find(attrs={"class": "entry-meta"})
				dtby_temp = dtby.text.split(" ")
			
				#authors, dates 다듬기
			
				for i in dtby_temp:
					#print(i)
					for j in month:
						if i == j:
							#print("find,",j)
							cut = count;
					count+=1
			
				for i in range(cut):
					#print(i)
					author+=dtby_temp[i+1]
					author+=" "
				
				for i in range(cut, len(dtby_temp)-1):
					date+=dtby_temp[i+1]
					date+=" "
				
				#authors, dates 출력
				
				if (cut>0):
					authors.append(author.strip())
					dates.append(date.strip())
				else:
					authors.append("ERROR")
					dates.append("ERROR")
					
					
	print(titles)
	#print(links)
	#print(dates)
	#print(authors)
	
	
def nydailynews():
	titles = []
	links = []
	dates = []
	authors = []
	contents = []
	
	page = requests.get('https://www.nydailynews.com/search/Korea/100-y/ALL/score/1/')
	soup = BeautifulSoup(page.content, 'html.parser')
	nydailynews = soup.select('div > div > div > div > div.h7 > a')
	
	for title in nydailynews:
		if ((title.parent.parent.find(attrs={"class": "tag tag-outline tag-first"}).text).strip()!='PHOTOS'):
			titles.append(title.text)
			templink = 'https://www.nydailynews.com'
			templink+= title.get('href')
			links.append(templink)
			timestamp = title.parent.parent.find(attrs={"class": "timestamp"})
			if (timestamp != None):
				dates.append(timestamp.text.strip())
			else:
				dates.append("Unknown")
				
		#deep for authors
		page = requests.get(templink)
		soup = BeautifulSoup(page.content, 'html.parser')
		temp = soup.find(attrs={"class": "byline byline-article"})
		if (temp != None):
			author = temp.find(attrs={"class": "uppercase"})
			authors.append(author.text.strip())
		else:
			authors.append("Unknown")
		
				
	
	print(titles)
	print(links)
	print(dates)
	print(authors)
	

	
usatoday()
#print("\n\n\n************ CHANGE ************\n\n\n")
#nypost()
#nydailynews()
